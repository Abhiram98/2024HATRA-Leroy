\section{Related Work}
\label{sec:relwork}
%-------------------------------------------------------------------------------
% \todo{Section 6, merging all group members drafts, below are the individual drafts}

% Abhiram: 
% \textbf{Program synthesis} aims to auto-generate programs that meet input-output requirements. Most techniques utilise a specialised DSL(domain specific language) to make the synthesis faster and more tractable. 
% Dreamcoder\cite{ellis2020dreamcoder} and EC2 \cite{EC2} lead a line of work\cite{laps, LILO, EC, MCMC} in inductive program synthesis, which takes inspirations from the way humans write code: developing reusable libraries. 
% These techniques take a set of tasks(ex: input, and corresponding outputs), and aim to synthesise programs at meet the task's specifications. To achieve this goal, a two-step process is followed. First, the technique partially synthesises a suite of programs that accomplish the tasks. Then, it learns reusable elements(libraries) from these programs which are further used to enhance the quality of synthesis. 
% % Specifically, DreamCoder uses an EC\textsuperscript{2} (Explore, Compress, Compile) ~\cite{EC2} synthesis algorithm, where it abstracts libraries in one of its 2 sleep stages to explore potential extensions to the given corpus, then compress the found abstractions in the next sleep stage to grow the library used for training the recognition neural network and synthesize programs in the wake stage of the next cycle. However, DreamCoder's success in solving problems in various domains, such as physics laws and text-editing, came with very high time cost. 
% This line of work is designed, or evaluated for learning abstractions over programs written in domain specific languages or lambda calculus. 
% % \todo{It is unclear how these techniques extend to higher-level languages.}
% % LILO uses LLMs to documentation, functional languages.
% % \todo{Add citations to LAPS, LILO, EC2, EC}

% Some works\cite{regal, patios} aim to extend these techniques to high-level languages. PATOIS \cite{patios}, equips and trains a neural synthesizer to use learned code idioms, while Regal\cite{regal} utilises LLMs to synthesise and abstract programs. Both of these techniques do not specifically aim to learn better abstractions.
% % These techniques are primarily 

% \textbf{Learning Abstractions:} 
% % that are useful and repeated in a corpus of programs is the goal of \cite{shapecoder, shapemod, babble, stitch}. 
% Babble\cite{babble} and Stitch\cite{stitch} are two closely related works that focus entirely on library extraction. While babble aims to improve the quality of learned libraries by using e-graphs and rewrite rules, stitch provides guarantees on the size of the learned corpus and the speed at which libraries are learned. 
% ShapeMod\cite{shapemod} and Shapecoder \cite{shapecoder} learn useful and explainable macros by limiting to programs that draw 3D shapes.
% % This line of work is designed, or evaluated for learning abstractions over domain specific languages(such as drawing shapes), or functional programming languages.
% Our work aims to extend these ideas to high-level languages like Python.


% An \textbf{Extract Method refactoring} \cite{jdeo, jextract, gems} aims to extract reusable blocks of code from a function. While these techniques operate on languages commonly used by software developers(Java), most have a limited vision: only the function to be modified is taken into account. In contrast, library learning techniques such a babble\cite{babble} look at the entire corpus of programs as a whole, with the potential to discover abstractions by correctly rewriting programs. 


\subsection{Program synthesis}
Library Abstraction aims to compress code via extracting common functionalities of multiple functions into a single repeatedly used function. However, this is usually done with the bigger goal of generating better libraries to be used for program synthesis (the generation of programs that solve a given problem). Most current state-of-the art work on library extraction are based on DreamCoder~\cite{ellis2020dreamcoder}, a program synthesizer that adopts the wake-sleep algorithm~\cite{wake-sleep} to bootstrap inductive program synthesis from a small problem-corpus represented with a Domain Specific Language (DSL). Specifically, DreamCoder uses an EC\textsuperscript{2} (Explore, Compress, Compile)~\cite{EC2} synthesis algorithm, where it abstracts libraries in one of its 2 sleep stages to explore potential extensions to the given corpus, then compress the found abstractions in the next sleep stage to grow the library used for training the recognition neural network and synthesize programs in the wake stage of the next cycle. However, DreamCoder's success in solving problems in various domains, such as physics laws and text-editing, came with very high time cost. 

\subsection{Library Abstraction}
Subsequent work aimed to enhance DreamCoder through enhancing the abstraction methods. For example, Babble~\cite{Cao_2023babble} proposes utilizing semantics-based abstractions, which results in better abstractions in less time. However, Babble's efficiency comes with an input complexity cost. In addition to the program corpus, users must also input a semantic equivalence list, which the program uses to abstract common libraries over semantically equivalent programs that look different. Stitch~\cite{Bowers_2023stitch} on the other hand aims to efficiently abstract libraries by reducing the abstraction search space. It does so by defining a utility measure, and performing a top-down search that optimizes utility using the branch and bound method, which eliminates search branches that are upper-bounded by a utility smaller than the current best utility. 

\subsection{Using LLMs}
As discussed earlier, one aim of library abstraction is to enhance program synthesis, which requires the abstracted libraries to be properly documented, which Lilo~\cite{grand2024lilo} covers. To improve program synthesis, Lilo utilizes Large Language Models (LLMs) to provide common sense knowledge as a first search step over the string space, before performing an enumerative search over the program space as in DreamCoder. Lilo then uses the Stitch compressor, and once more utilize LLMs to document the resulting libraries with a natural language. The use of the full lilo system enhances performance, in contrast to incorporating LLM guided search without the documentation phase which degrades performance. However, all aforementioned work take in \(lambda\)-calculus and DSL input, unlike ReGAL~\cite{stengeleskin2024regal} which similarly uses LLM-guided search and utilize LLMs in library generation and documentation, but also works with general purpose languages like python. However, ReGAL uses library extraction as a step in program synthesis, where its main goal is to synthesize programs that satisfy a given written task. 

\subsection{Visual Abstraction, Program Learning, and Rewriting}
Other works cover library abstractions with a focus on visual representation of programs~\cite{jones2023shapecoder} ~\cite{wang2021learningVisAbst}~\cite{Jones_2021}. ShapeCoder ~\cite{jones2023shapecoder}  for example uses Neural Networks and e-graphs to not only extract useful abstractions from visual modeling of programs, but also explain input shapes using the abstractions. 
Other lines of work, despite not sharing the library extraction focus, share components of programming and natural language processing and recognition. This includes work on program learning~\cite{cropper2019playgol}~\cite{DBLP:journals/corr/abs-2004-09931refproginduc}~\cite{wong2022leveraging}  ~\cite{demo}~\cite{iyer2019learning} ~\cite{hocquette2024learning} and program rewriting~\cite{brandfonbrener2024verified} ~\cite{DBLP:conf/sat/NotzliRBNPBT19rewrite}~\cite{ganeshan2023improving}, which use relevant methods such as finding idioms, and search algorithms as the Monte Carlo tree Search. 
